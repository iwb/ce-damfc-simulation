import pandas as pd
import os

# Access output directory
base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
output_dir = os.path.join(base_dir, 'output')

class ProcessCSV:
    """
    This class is used to process the csv files generated by the simulation.
    Two .csv are generated: log_order_specific.csv and log_stations_specific.csv
    """
    def __init__(self, log_events=os.path.join(output_dir,"log_events.csv"), kpi_orders=os.path.join(output_dir,"summary_orders.csv"), kpi_stations=os.path.join(output_dir,"summary_stations.csv"),
                 log_order_specific=os.path.join(output_dir,"log_order_specific.csv"), log_station_specific=os.path.join(output_dir,"log_station_specific.csv")):
        """
        Initialize the CSVProcessor with the file paths for the log files and summary files.

        Parameters:
        log_events (str): Path to the log events file.
        kpi_orders (str): Path to the orders kpi table.
        kpi_stations (str): Path to the stations kpi table.
        log_order_specific (str): Path for saving the summary file for orders.
        log_station_specific (str): Path for saving the summary file for stations.
        """
        
        self.log_events = log_events
        self.kpi_orders = kpi_orders
        self.kpi_stations = kpi_stations
        self.log_order_specific = log_order_specific
        self.log_station_specific = log_station_specific
        
        # Clear the contents of orders, and stations summary files
        open(self.log_order_specific, 'w').close()
        open(self.log_station_specific, 'w').close()

    def process_and_save_logs(self):
        """
        This function processes and saves summaries for orders and stations.
        """
        self.create_log_order_specific()
        self.create_log_station_specific()

        
    def create_log_order_specific(self):
        """
        This method processes log data to generate an orders summary.
        It reads log files, processes each order's data, calculates task durations,
        merges with order information, and saves the summary to a CSV file.
        """
        # Load log data
        if os.path.exists(self.log_events):
            df_events = pd.read_csv(self.log_events)
        else:
            print(f"{self.log_events} does not exist. Please check the path.")
            return
        
        if os.path.exists(self.kpi_orders):
            df_orders = pd.read_csv(self.kpi_orders)
        else:
            print(f"{self.kpi_orders} does not exist. Please check the path.")
            return
        
        processed_data = []
        # Group by simulation and order
        for (simulation_id, order_id), group in df_events.groupby(['Simulation ID', 'Order ID']):
            order_data = {
            "Simulation ID": simulation_id,
            "Rules": group['Rules'].iloc[0],
            "Order ID": order_id,
            }

            # Track task times dynamically
            tasks = {}
        
            for _, row in group.iterrows():
                event_type = row['Event Type']
                task_name = row['Task Name']
                timestamp = row['Timestamp']
         
                if event_type == "Task Start":
                    # Create entry for task if it doesn't exist
                    if task_name not in tasks:
                        tasks[task_name] = {}
                    tasks[task_name]["Start Time"] = timestamp
                    
                elif event_type == "Task Complete":
                    # Ensure task exists and record complete time
                    if task_name not in tasks:
                        tasks[task_name] = {}
                    tasks[task_name]["End Time"] = timestamp
                    
                    # Calculate task duration if start time exists
                    if "Start Time" in tasks[task_name]:
                        process_time = tasks[task_name]["End Time"] - tasks[task_name]["Start Time"]
                        tasks[task_name]["Duration"] = process_time
                        #order_data["Process Time"] += process_time

            # Flatten task information into columns
            for i, (task_name, times) in enumerate(tasks.items(), start=1):
                order_data[f"Task{i}_Name"] = task_name
                order_data[f"Task{i}_Start"] = times.get("Start Time", None)
                order_data[f"Task{i}_End"] = times.get("End Time", None)
                order_data[f"Task{i}_Duration"] = times.get("Duration", None)
            
            
            # Append the processed order data to the list
            processed_data.append(order_data)

        # Convert processed data to a DataFrame
        df_orders_log = pd.DataFrame(processed_data)

        # Merge with order log data on Simulation ID and Station ID
        df_combined = pd.merge(df_orders_log, df_orders, on=["Simulation ID", "Rules", "Order ID"], how="left")

        # Save the summary data and average lead time to CSV
        df_combined.to_csv(self.log_order_specific, index=False)
       
    
    def create_log_station_specific(self):
        """
        This function processes event logs to generate a summary of station activities.
        It reads event and station log data, calculates task processing times, and merges the data to create a comprehensive summary.
        The summary is then saved to a CSV file.
        """

        # Load the raw event log data
        if os.path.exists(self.log_events):
            df_events = pd.read_csv(self.log_events)
        else:
            print(f"{self.log_events} does not exist. Please check the path.")
            return
        
         # Load station log data
        if os.path.exists(self.kpi_stations):
            df_stations = pd.read_csv(self.kpi_stations)
        else:
            print(f"{self.kpi_stations} does not exist. Please check the path.")
            return
        
        
        # Initialize a list to hold processed data for each station
        task_data = []
        # Group by simulation and station
        for (simulation_id, rules, station_id), group in df_events.groupby(['Simulation ID', 'Rules' ,'Station ID']):
            station_info = {
                "Simulation ID": simulation_id,
                'Rules':rules,
                "Station ID": station_id,
                "Tasks": []  # Store details of each task
            }

            for _, row in group.iterrows():
                event_type = row['Event Type']
                order_id = row['Order ID']
                task_name = row['Task Name']
                timestamp = row['Timestamp']
                details = row['Details']
                
                # Process time calculation for Task Start events
                if event_type == "Task Start":
                    # Record task start time and details
                    task_info = {
                        "Order ID": order_id,
                        "Task ID": task_name,
                        "Start Time": timestamp,
                        "End Time": None  # Will be updated upon Task Complete
                    }
                    station_info["Tasks"].append(task_info)
                
                # Update task end time for Task Complete events
                elif event_type == "Task Complete":
                    # Find the last task entry for this task and update its end time
                    for task in station_info["Tasks"]:
                        if task["Order ID"] == order_id and task["Task ID"] == task_name and task["End Time"] is None:
                            task["End Time"] = timestamp
                            break
            
            # Append the processed station info to the station data list
            task_data.append(station_info)

        # Prepare final DataFrame with flattened task data
        expanded_data = []
        for info in task_data:
            tasks_flat = []
            for i, task in enumerate(info["Tasks"], start=1):
                tasks_flat.extend([
                    (f"Task{i}_Order_ID", task["Order ID"]),
                    (f"Task{i}_ID", task["Task ID"]),
                    (f"Task{i}_Start_Time", task["Start Time"]),
                    (f"Task{i}_End_Time", task["End Time"])
                ])
            
            # Create a row for each station with flattened task data
            station_row = {
                "Simulation ID": info["Simulation ID"],
                "Rules": info["Rules"],
                "Station ID": info["Station ID"],
            }
            # Add task data as individual columns
            station_row.update(dict(tasks_flat))
            expanded_data.append(station_row)

         # Convert processed station data to DataFrame
        df_stations_task_data = pd.DataFrame(expanded_data)

        # Merge with station log data on Simulation ID and Station ID
        df_combined = pd.merge(df_stations_task_data, df_stations, on=["Simulation ID", "Rules", "Station ID"], how="left")

        # Save the final combined summary to a CSV file
        df_combined.to_csv(self.log_station_specific, index=False)

        

